# VisionTransformer-PyTorch-Reproducing-the-ViT-Paper-From-Scratch-Pretrained
A PyTorch implementation replicating the Vision Transformer (ViT) paper.
This project explores both a from scratch implementation and experiments using torchvisionâ€™s pretrained ViT models, including the DEFAULT and SWAG pretrained weights.

ğŸš€ Project Overview

ğŸ”¥ Implemented Vision Transformer (ViT) architecture from scratch using PyTorch
ğŸ§  Used torchvision pretrained ViT (DEFAULT) and SWAG-pretrained ViT as feature extractors
ğŸ“Š Trained all models on 20% of the dataset for fair comparison
ğŸ“ˆ Visualized loss and accuracy curves to analyze convergence and generalization

ğŸ“ˆ Results Summary

ViT (DEFAULT Pretrained)
âœ… Final Train Accuracy: 98.05%
âœ… Final Test Accuracy: 93.89%
ğŸ† Best Test Accuracy: 97.66% at Epoch 5
ğŸ• Epochs: 10

ViT (SWAG Pretrained)
âœ… Final Train Accuracy: 98.22%
âœ… Final Test Accuracy: 98.67%
ğŸ† Best Test Accuracy: 98.67% (Epochs 6â€“10)
ğŸ• Epochs: 10

ğŸ’¡ Insights

ğŸ”¹ Both pretrained models achieved strong generalization with minimal overfitting
ğŸ”¹ The SWAG pretrained ViT converged faster and reached higher accuracy with fewer epochs
ğŸ”¹ Results align with the findings from the original ViT paper, highlighting the power of large scale pretraining for efficient fine-tuning
